Papers
------------------------

Contributed by Weiming Hu

### Data Type

| Conf.     | **Title**                                                    | **Key Words**                               |
| --------- | ------------------------------------------------------------ | ------------------------------------------- |
| ASPLOS'24 | 8-bit Transformer Inference and Fine-tuning for Edge Accelerators | Posit8 and FP8                              |
| DAC'24    | Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference | Posit                                       |
| ICLR'24   | Block and Subword-Scaling Floating-Point (BSFP) : An Efficient Non-Uniform Quantization For Low Precision Inference | Propose BSFP                                |
| NIPS'24   | QLoRA: Efficient Finetuning of Quantized LLMs                | Propose NormalFloat4                        |
| ISCA'23   | With Shared Microexponents, A Little Shifting Goes a Long Way | Microsoft MX-FP (two-level shared exponent) |
| arXiv'23  | Microscaling Data Formats for Deep Learning                  | Explore MXFP4, 6, 8                         |
| MICRO'22  | ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization | Adaptive data type, propose Flint           |
| HPCA'22   | FAST: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding | Use BFP in training                         |
| ICML'22   | Be Like Water: Adaptive Floating Point for Machine Learning  | Adaptive floating point                     |
| NIPS'20   | Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point | First propose MSFP                          |

#### Logarithemic Number System

| Conf.   | **Title**                                                    | **Key Words**      |
| ------- | ------------------------------------------------------------ | ------------------ |
| DAC'24  | Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference | Logarithmic, posit |
| TETC'24 | A Design Framework for H Efficient Logarithmic Floating-Point Multipliers | LFP Multipliers    |
| VLSI'22 | Half-Precision Logarithmic Arithmetic Unit Based on the Fused Logarithmic and Antilogarithmic Converter | Log converter      |
| TC'22   | LNS-Madam: Low-Precision Training in Logarithmic Number System Using Multiplicative Weight Update | LNS training       |

#### MX Format

| Conf.    | **Title**                                                    | **Key Words**                 |
| -------- | ------------------------------------------------------------ | ----------------------------- |
| arXiv'24 | Nanoscaling Floating-Point (NxFP): NanoMantissa, Adaptive Microexponents, and Code Recycling for Direct-Cast Compression of Large Language Models |                               |
| arXiv'24 | AMXFP4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point for 4-bit LLM Inference | MXFP, Asymmetric quantization |
| arXiv'24 | MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization | MXINT, MXFP, outlier          |
| arXiv'24 | Post Training Quantization of Large Language Models with Microscaling Formats | MXINT                         |
| arXiv'24 | Error Diffusion: Post Training Quantization with Block-Scaled Number Formats for Neural Networks | error compensation            |
| arXiv'23 | A Dataflow Compiler for Efficient LLM Inference using Custom Microscaling Formats | code template                 |
| ICLR'23  | Block and Subword-Scaling Floating-Point (BSFP) : An Efficient Non-Uniform Quantization For Low Precision Inference | BSFP                          |
| ISCA'23  | With Shared Microexponents, A Little Shifting Goes a Long Way | MX-FP, two-level scaling      |
| arXiv'23 | Microscaling Data Formats for Deep Learning                  | Explore MXFP4, 6, 8           |
| HPCA'21  | FAST: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding | BFP training                  |
| NIPS'20  | Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point | MSFP (BFP)                    |

